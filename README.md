# ğŸš€ Automated Real-Time Data Streaming Pipeline  
*Using Apache NiFi, AWS S3, Snowflake Snowpipe, Streams, and Tasks* ğŸ¯

---

## ğŸ” Overview

This project demonstrates the design and implementation of a **real-time data streaming pipeline** that continuously ingests, processes, and transforms data with minimal latency. Leveraging modern tools and cloud services, this pipeline automates the flow from data generation to analytics-ready datasets in Snowflake. â„ï¸âœ¨

---

## ğŸ—ï¸ Architecture & Workflow

![UI_2](architecture.png "Sample_1")

- **ğŸ› ï¸ Data Generation:**  
  Synthetic customer data created with Python and the `Faker` library inside JupyterLab.

- **ğŸ›ï¸ Apache NiFi:**  
  Orchestrates and streams generated data to an AWS S3 bucket for scalable cloud storage.

- **â˜ï¸ AWS S3:**  
  The landing zone for raw data files, triggering Snowpipe ingestion automatically.

- **â„ï¸ Snowflake Snowpipe:**  
  Detects new files in S3 and ingests them near real-time into Snowflake staging tables.

- **ğŸ”„ Snowflake Streams & Tasks:**  
  - *Streams* track changes in staging tables (Change Data Capture).  
  - *Tasks* automate incremental transforms and upserts into target tables.

---

## ğŸ¯ Key Components

| Component         | Description                                      |
|-------------------|------------------------------------------------|
| **ğŸ–¥ï¸ AWS EC2**       | Virtual server hosting Docker container and tools |
| **ğŸ³ Docker**        | Containerized environment with Python, NiFi, ZooKeeper |
| **ğŸ““ JupyterLab**    | Workspace for data generation and experimentation  |
| **ğŸ›ï¸ Apache NiFi**   | Real-time data flow orchestration and routing  |
| **â˜ï¸ AWS S3**        | Cloud storage for raw data files                |
| **â„ï¸ Snowflake**     | Cloud data warehouse for ingestion & analytics |
| **âš¡ Snowpipe**      | Serverless continuous data ingestion service   |
| **ğŸ”„ Streams & Tasks** | Snowflake CDC and automation features           |

---

## âš™ï¸ Project Setup Summary

1. **ğŸ–¥ï¸ EC2 Configuration:**  
   Set up an AWS EC2 instance (t2.xlarge) with 8GB RAM and 32GB storage. Opened ports for JupyterLab and NiFi.

2. **ğŸ³ Docker Setup:**  
   Created Docker container on EC2 with Python, Apache NiFi, and ZooKeeper for consistent environments.

3. **ğŸ““ JupyterLab & ğŸ›ï¸ NiFi:**  
   Configured to run on ports `4888` and `2080` respectively for easy web access.

4. **ğŸ² Data Generation:**  
   Python scripts using Faker to simulate streaming customer data inside JupyterLab.

5. **ğŸšš Data Streaming:**  
   NiFi configured to push data in real-time to AWS S3 bucket.

6. **â„ï¸ Real-Time Processing:**  
   Snowpipe auto-ingests data; Streams and Tasks handle change tracking and incremental updates.

---

## ğŸŒŸ Project Highlights

- **âš¡ Real-time automated ingestion and processing** with Snowpipe, Streams & Tasks  
- **â˜ï¸ Scalable architecture** using AWS S3 and Snowflake  
- **ğŸ³ Containerized & reproducible environment** with Docker  
- **ğŸ¼ End-to-end orchestration** powered by Apache NiFi  

---

## ğŸŒ Access URLs

- **ğŸ““ JupyterLab:** http://`<EC2-IP>`:4888  
- **ğŸ›ï¸ Apache NiFi:** http://`<EC2-IP>`:2080  

*(Replace `<EC2-IP>` with your instance's public IP)*

---

## ğŸ’¡ Technologies & Skills Demonstrated

- Python & Faker for data simulation  
- Apache NiFi for data flow orchestration  
- AWS S3 cloud storage integration  
- Snowflake Snowpipe for serverless ingestion  
- Snowflake Streams & Tasks for CDC & automation  
- Docker containerization  
- Cloud infrastructure setup & security  

---

## ğŸ‰ Conclusion

This project successfully builds an automated, end-to-end **real-time data streaming pipeline** that efficiently moves data from generation to analytics-ready datasets. It highlights modern best practices in cloud data engineering and real-time analytics. ğŸŠ

---


